{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exploratory Data Analysis\n",
    "\n",
    "### **Objective:** \n",
    "\n",
    "Every data science project starts with the following steps:\n",
    "\n",
    "1. Data Loading \n",
    "2. Data Cleaning\n",
    "3. Data Preparation\n",
    "4. Exploratory Data Analysis\n",
    "\n",
    "In this exercise we load a dataset from a url, clean the data by assign feature names and check for missing values, preparing the data by creating new features and conduct exploratory data analysis by answering questions about the data.\n",
    "\n",
    "### **Dataset:**\n",
    "\n",
    "Auto MPG dataset from the UCI Machine Learning Repository. The dataset contains 398 samples with 8 features, and the\n",
    "target variable is the fuel efficiency in miles per gallon. (https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data)\n",
    "\n",
    "1. **Data Loading**\n",
    "    \n",
    "    1.1 Import the necessary libraries: pandas and numpy.\n",
    "    \n",
    "    1.2 Load the auto-mpg dataset from the given url.\n",
    "\n",
    "2. **Data Cleaning**\n",
    "\n",
    "    2.1 Assign the following column names to the dataset: [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\",\"acceleration\", \"model_year\", \"origin\", \"car_name\"]\n",
    "\n",
    "    2.2 Get basic information about the data and check for missing values.\n",
    "\n",
    "3. **Data Preparation**\n",
    "\n",
    "    3.1 Check the data types and adjust if needed.\n",
    "    \n",
    "    3.2 Create a new feature called \"bestsellers\" that flags all cars with 8 cylinders and model 70 with 1, all cars with 6 cylinders and model year 80 with 2, and the rest with 0.\n",
    "\n",
    "4. **Exploratory Data Analysis**\n",
    "\n",
    "    4.1 What is the average *horse power* and minimum *weight* of the cars in this dataset?\n",
    "\n",
    "    4.2 What is the maximum *mpg* per *model year* and *cylinder*? \n",
    "\n",
    "    4.3 How many cars weigh less than 3449 kg and have more than 5 cylinders?\n",
    "\n",
    "#### **Hints/reminders:**\n",
    "\n",
    "- pd.read_csv(url)\n",
    "- data.columns = [\"a\", \"b\", … ] renames columns to ’a’, ’b’, etc.\n",
    "- pd.to_numeric(data[\"a\"] errors='coerce') converts columns a to numeric data or to NaN if conversion is not possible\n",
    "- data = data.dropna() drops all rows that contain NaN\n",
    "- remember np.mean() and np.abs()\n",
    "- see np.where()\n",
    "- check the pandas function df.groupby & df.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"c:/data/iris.csv\")\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "X = data.drop('species',axis=1)\n",
    "Y=data['species']\n",
    "\n",
    "model.fit(X,Y)\n",
    "res = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(30,10))\n",
    "tree.plot_tree(model,\n",
    "feature_names=X.columns.tolist(),\n",
    "class_names=model.classes_.tolist(),\n",
    "filled=True,fontsize=10)\n",
    "text = tree.export_text(model)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_tree\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris\n",
    "\n",
    "iris.feature_names\n",
    "\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data\n",
    "\n",
    "X = data.iloc[:, 0:3]\n",
    "y = data.iloc[:, [3]]\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "pred = model.predict(Xtest)\n",
    "pred\n",
    "\n",
    "plt.scatter(Ytest, pred)\n",
    "plt.xlabel(\"actual\")\n",
    "plt.ylabel(\"predicted\")\n",
    "\n",
    "mae = metrics.mean_absolute_error(Ytest, pred)\n",
    "print('MAE: ' + str(mae))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data from a file\n",
    "data = pd.read_csv('C:\\\\data\\\\churn_exercise.csv')\n",
    "\n",
    "# Split the data into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['usage_hours', 'complaints']], data.customer_left, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create a Decision Tree Classifier model\n",
    "dt_model = DecisionTreeClassifier(max_depth=3,criterion='entropy') #limiting tree size\n",
    "#dt_model = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# Train the model on the training data\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_model, filled=True, feature_names=X_train.columns.tolist(), class_names=['No Churn', 'Churn'])\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "#accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy=sum(y_pred==y_test)/len(y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "confusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error  # ← для оценки MAE\n",
    "\n",
    "# Загрузка данных\n",
    "data = pd.read_csv(\"houses.csv\", header=None)\n",
    "data.columns = ['living_space', 'size_of_property', 'price']\n",
    "\n",
    "x = data.living_space\n",
    "y = data.price\n",
    "\n",
    "# Полиномиальная регрессия степени 5\n",
    "degree = 5\n",
    "model = np.poly1d(np.polyfit(x, y, degree))\n",
    "\n",
    "# Предсказания\n",
    "y_hat = model(x)\n",
    "\n",
    "# Подготовка данных для построения кривой\n",
    "print_data = np.linspace(0, np.max(x), 100)\n",
    "\n",
    "# Визуализация\n",
    "plt.figure()\n",
    "plt.scatter(x, y)\n",
    "plt.plot(print_data, model(print_data), c=\"black\")\n",
    "#plt.ylim(min(data.price) * 0.8, max(data.price) * 1.2)\n",
    "plt.xlabel(\"Living space (m²)\")\n",
    "plt.ylabel(\"Price (€)\")\n",
    "plt.title(f\"Polynomial Regression (Degree {degree})\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Оценка ошибки MAE\n",
    "mae = mean_absolute_error(y, y_hat)\n",
    "print(f'The MAE is {mae:.2f}')\n",
    "\n",
    "# Предсказание цены для площади 280 м²\n",
    "house280 = model(280)\n",
    "print(f'The prediction for a house of size 280 is {house280:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Fish.csv')\n",
    "\n",
    "X = data.iloc[: , 2 : ]\n",
    "Y = data.iloc[:, [1]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size= 0.7, shuffle=True)\n",
    "\n",
    "model = XGBRegressor()\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "\n",
    "plt.scatter(y_test, pred)\n",
    "plt.xlabel('actual')\n",
    "plt.ylabel('predicted')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "print(f\"MAE is: {mae}\")\n",
    "\n",
    "baseline = np.repeat(y_train.mean().values, len(y_test))\n",
    "\n",
    "print('MAE comparing by baseline is: ' , mean_absolute_error(y_test, baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.feature_names\n",
    "\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "X = data\n",
    "iris.target_names\n",
    "y = iris.target == 1\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=2, shuffle=True)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "\n",
    "pred = model.predict(Xtest)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(Ytest, pred))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(Ytest, pred).ravel()\n",
    "\n",
    "pred_proba = model.predict_proba(Xtest)\n",
    "pred_proba\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Ytest, pred_proba[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('responses.csv', sep=',')\n",
    "\n",
    "df_num = df.select_dtypes(exclude='object')\n",
    "\n",
    "df_num = df_num.dropna()\n",
    "\n",
    "X = df_num.drop(columns=['Age'])\n",
    "y = df_num['Age'] >= 19\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "\n",
    "pred = model.predict(Xtest)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(Ytest, pred))\n",
    "\n",
    "tn, fp, tp, fn = confusion_matrix(Ytest, pred).ravel()\n",
    "\n",
    "pred_proba = model.predict_proba(Xtest)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(Ytest, pred_proba[:, 1])\n",
    "\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
